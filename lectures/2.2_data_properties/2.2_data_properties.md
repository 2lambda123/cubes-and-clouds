# Data Properties

## Learning objectives

- Learn what are metadata and why they are important
- Discover data properties and how to filter by them

## What are data properties and metadata

Data properties and metadata are sets of characteristics of geospatial data that serve different purposes. Commonly data properties are seen as attributes describing the image itself whereas, on the other hand, metadata describes the properties of a product.

**Data Properties:** Data properties refer to the characteristics and attributes of the geospatial data itself. These properties describe the actual content, structure, and values within the dataset. For raster data, properties can include information such as pixel values, cell size, spatial resolution, number of bands, and data format. For vector data, properties may include attributes associated with points, lines, or polygons, such as feature identifiers, names, classifications, or numerical values. Data properties are essential for understanding the data at a fundamental level and are used in the analysis, visualization, and processing tasks.  

**Metadata:** Metadata, on the other hand, provides descriptive information about the data product. It serves as documentation that complements the data properties and offers additional context and details about the data. Metadata describes the origin, and characteristics, and helps with usage of the data, allowing users to discover, evaluate, and effectively utilize the data. It typically includes information such as the data source, acquisition parameters, coordinate reference system, temporal coverage, quality measures, and data access policies. Metadata plays a crucial role in data management, data sharing, and data interoperability by providing the necessary documentation and context for understanding and utilizing the data.

Both together provide a complete description of the data of our interest itself and how to work with them. Standardized ways how to describe metadata and data properties allow geospatial systems to work with data easily.

## Properties and metadata used for filtering

In [previous lesson](../2.1_data_discovery/2.1_data_discovery.md) we learned where to find data. Now it is time to look at how we can select only the data we want. Properties and metadata are a great help when we are not interested in the whole collection, but only in certain regions, times or even bands of selected satellite products.

In (many of) the data catalogs, we can filter by specific values for each satellite. Let's talk more about some of them shortly

- **Dataset name/ Identifier:** Filtering directly by name of a product as a unique name or identifier is assigned to the dataset, allowing it to be easily identified and referenced.
- **Time range:** The temporal coverage or specific dates associated with the data acquisition. By this, we can easily select products from the same location with different dates of acquisition. This is particularly relevant for time-series or multi-temporal datasets.
- **Bounding box or other area of interest:** Spatial Extent or the geographic coverage of the raster data, typically defined by the bounding coordinates (longitude and latitude) that encompass the dataset. Usually you can select your own area or interest as a rectangle or use a map window for taking the current extent
- **Mission:** You can select by satellite used for data acquisition. In the case of Sentinel missions, you can select only Sentinel-1 as an example
- **Processing levels:** Typically you can also select by processing level you are interested in for your missions. Typically Level-0 means unprocessed, raw data, and the higher the number of represents more corrections applied.
- **Sensors or instrument:** Selection by the output of a specific sensor or instrument.
- **Cloud coverage/polarization**: Based on a mission, a selection filter can be made by specific parameters. For Sentinel-2 data, we can typically filter by cloud coverage in percentage. This is used as we are interested in lower cloud coverage percent for analysis. Similar can be used for filtering only polarization we are interested in Sentinel-1 missions.
- **Orbit number:** Typically integer number selects a specific orbit number user is interested in.
- **Orbit direction:** For the majority of data types is selection either ascending or descending.
- **Availability status/Timeliness:** Some missions have different time availability for their data allowing them to select Near Real-Time acquisitions (approximately 3 hours after acquisition), raw data with a certain delay, or processed data later. It is also common that infrequently accessed data or older data than a certain threshold (years) can only be accessed 'on demand'. Their 'Availability status' is usually set as 'Archived' or similar and they but must be 'tasked to be brought online from archive storage' by the user. This operation usually takes minutes to hours to complete. The user of the platform can later access the catalog again and hopefully the product have been brought online in the meantime.

#### Animated Content: STAC Standardizing Metadata (animated picture)


### Metadata and properties which are not typically used for filtering

Many metadata are contained within the product but are not used for filtering on the platforms or accessing hubs directly. Among these is for example Author of the dataset or Licence. You should be able to get all information about those in data metadata when you are accessing the data itself or on the general page with information.

Usually it is unfortunatelly not possible to filter by or search by direct data properties, e.g. by values in data. 

#### Animated Content: Selection what we are interested in for our workflow and then find out which filtering parameters we need

### Dimensions

More information about dimensions of data is in the lecture about [Datacubes](../1.2/) 
**To Do: based on decision where to put this**

- x,y (z) - Spatial dimension of data
- temporal/time dimension - capturing the time aspect for time series analysis

### Value Types (data types)

Once we have selected data products we are interested in, we can look directly into values to select what we are interested in. Common data types representing measured values are these:

- bitmask 0/1
- 8bit 0-255
- UInt16 - 0-65k
- Int16 - -32k - 32k
- Float32

**To Do: Decide if relevant here: GDALInfo for extracting data**

## Exam

- Fill STAC item for a data product
- How many products we can find on CDSE for Sentinel-2 L1C between 14. 5. - 26. 5 2023 with 20% cloud coverage over Sardinia?
- Is the same number of products on *other* platform?
- Extract metadata from the openEO Platform, e.g. make a histogram of the resolutions on the openEO platform for a specific date
- Download one file and use GDALInfo on it to check:
  - Spatial extent
  - Band data types
  - NODATA values
  - Coordinate system
  - Raster size
- Calculate the volume of data needed for the snow workflow according to the collections, time steps, bands, resolutions, spatial extent selected in the previous chapter, value types, format, compression estimate, etc. (this only if we define snow workflow already here)
- What doesn't belong to Data Metadata - e.g. measured values from sensor
- What doesn't belong to Data Properties - eg. Licence information

## Compression Quiz

Consider a cloud provider, that should decide if compressing the data on its storage would let it spare money.

If the compression process of a COG takes 0.05 CPU hour every 1 GB of data, the total amount of COGs on the storage takes 1200 TB and one CPU hour costs 0.05€ for the first 10000 CPU hour and then 0.03€ for the rest, how much would the compression process cost?

    [( )] 3000€
    [( )] 1200€
    [(X)] 2000€
    [( )] 2043€

Solution:
- 1 TB = 1000 GB following the international standard, see https://en.wikipedia.org/wiki/Gigabyte.
- The amount of data on the storage is 1200 TB * 1000 GB/TB = 1200000 GB
- If to compress 1 GB of data takes 0.05 CPU hour, to compress 1200000 GB it will take: 1200000 GB * 0.05 CPU hour / GB = 60000 CPU hour
- The first 10000 CPU hour cost 10000 CPU hour * 0.05 €/CPU hour = 500€
- The rest is 50000 CPU hour and it will cost 50000 CPU hour * 0.03 €/CPU hour = 1500 €
- The total is 500 € + 1500 € = 2000 €

In the same scenario, now we also consider the storage maintanance cost. If each TB of storage has a maintenance cost of 0.5 € every month, how much time would it take before the compression would let the cloud provider spare money? Consider a compression rate of 70%.

    [( )] ~ 9 months
    [( )] ~ 10 months
    [(X)] ~ 11 months
    [( )] ~ 12 months

Solution:
- Without compression, every month the storage would cost: 1200 TB * 0.5 €/TB = 600 €
- With a compression rate of 70%, the data would use 1200 TB * 0.7 = 840 TB of disk space
- With compression, every month the storage would cost: 840 TB * 0.5 €/TB = 420 €
- With compression, each month it's possible to spare 180 €.
- Since the compression process costs 2000 €, and each month costs 180 € less with it, it would take 2000 € / 180 €/month ~= 11 months to start to spare money.
