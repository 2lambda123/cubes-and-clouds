# 3.1 Data Processing

## Learning Objectives
- Carry out an EO workflow on a cloud platform
- Select suitable data
- Chain processes to form an EO processing chain/workflow
- Visualize the results

## Introdcution
In this lecture we are going to combine the knowledge and hands-on experience we have gathered so far to create a full EO workflow on a cloud platform.
We will  
- define a research question,
- choose and load the necessary data sources, 
- define the data cube to our needs, 
- use functions to process the data, 
- visualize the result
- and track the resources we are consuming on the platform.

## Case Study: Snow Cover in the Alps

#### Video: Advantages of a workflow on a cloud platform

### Research Question
Snow serves as a water reservoir and is thus important for any hydrological management activity, such as irrigation planning, drink water supply or hydro power generation. Knowing precisely, when and where snow is present is a critical source of information for these acitivities. Satellite earth observation plays an important role in describing the snow cover, both globally and in local mountain ranges. Due to it's ability to sense spatially covering it and repeatedly. Our goal is to create a time series of the snow covered area of the catchment of interest. We will use this time series to compare it to the run off at the main outlet of the catchment. And study the relationship between snow dynamics and runoff.

### Approach
In this exercise we are going to derive the snow cover in an alpine catchment using Sentinel-2 data. Sentinel-2 carries an optical sensor, it is measuring the reflected light of the earths surface in differenct wavelenghts. At a 10m spatial resolution and at a 6 day repeat rate. We are using the X and X band to calculate the Normalized Difference Snow Index (NDSI). It is calculated as follows: XXX. It makes use of the XXX phenomenon and expresses it as a formula. The result is a value between -1 and 1. The higher the value is, the more probable it is that the surface is covered with snow. In order to create a binary snow map we apply a threshold of NDSI < 0.4. This is a commonly used value for discriminating snowcovered and snow free areas. Then we spatially aggregate the snow free and snow covered pixels in the catchment area by summing them up. In order to get the snow covered area of the catchment we multiply the number of snow covered pixels by 10 m, which is the resolution of the Sentinel-2 data we are using. Additionally, we have to deal with cloud cover. We use the Sentinel-2 cloud mask that is provided with the data and exclude all images that have a cloud cover over XX % in our study area. Ideally we should fill the gaps the clouds generate, since they are introducing uncertainty. Nevertheless, for a first try our approach should be good enough to get a general idea about the snow cover in our area of interest. In the end we receive a time series with the snow covered area in the catchment. 

### Limitations and Improvements
Now that we have postulated a very basic approach to solve our research question we should take some time identify possible improvements to our fairly simple approach:
- Optical earth observation has some inherent drawbacks, most importantly: clouds. Especially in mountain regions.
  - We are excluding images where a certain cloud coverage is exceeded. There would still be some information available.
  - We are not filling in the gaps that clouds generate. This leaves us with some uncertainty.
  - Use data fusion techniques and include SAR data, that can penetrate the clouds. 
- Sentinel-2 has a 6 day repeat rate. This means we do not know what happens with the snow cover in between two acquisitions.
  - Use data fusion techniques and include other optical sensors and SAR data
  - Use physical snow models or heuristics to estimate the snow cover in between
- We are using a threshold for discriminating between snow and no snow. Changing this arbitrary value will influence our results.
  - There are better, more complex ways to identify snow.
- Snow Cover does not represent the amount of snow.
  - Therefore we would need to calculate the snow depth.
  - Or better the Snow Water Equivalent.

### Excursion: Using multiple Data Sources on a cloud platform. Data Fusion
What is to be considered when doing data fusion:
- Extent (time, space, bands)
- Resolution (time, space, bands; what is the information in the bands - is it comparable...)
- Dimensionality
- illustrate this with data cube models

How to solve this on a cloud platform!
- Checking the extent of the collections
- Aligning the extent of the collections
- Aligning the resolution of the collections (aggregating, resampling, reducing)

### Workflow Description
- Describe the functions we are using one by one, and what happens to the data cube.
- embed process graph as html

- get data:
  - `load_collection()`
- ndsi:
  - `filter_bands()`, `reduce_dimension()`
  - creates a -1 to 1 map, 1 signifies high amount of snow
- create binary snow classification (by threshold):
  - `mask()`, `gt()`
  - create a binary snow classification: 0 = no snow, 1 = snow
- cloud masking:
  - `mask()`
  - Apply the mask to the binary snow map: 0 = no snow, 1 = snow, NA = cloud
- snow covered area:
  - aggregate_spatial(), sum()
  - sum up all pixels, the sum corresponds to the snow covered area (*10m2), divide by area of catchment to get percentage
  - save as json time series: sca (or merge cubes with cld and save then)
- cloud percentage:
  -  aggregate_spatial(), count() or sum()
  -  count the number of NA pixels, divide by the total number of pixels = cloud percentage
  -  save as json time series: cld (or merge cubes with sca and save then)
- filter timeseries according to cloud coverage:
  - join the two timeseries by date, filter the dates that have cloud coverages > 30%
- compare to runoff time series:
  - load the runoff timeseries and plot the two time series against each other


### Excursion: How openEO functions are defined (input, arguments, output) 
Speaker Jeroen Dries, Matthias Mohr

## Tracking resources
- Explain Basics of resource usage on cloud platforms
- Point to Scaling Lesson: There we can go more into details

## Hands On Exercise
Now we have covered the most important topics of our use case in theory. Let's move on to produce some results!

**Disclaimer: The applied workflow is a simple approach used for educational reasons to learn how to use EO cloud platforms**

#### Exercise: SCA timeseries

## Quiz
### Theory Part
- Which parts are necessary to define an openEO process (is present in every openEO process definition)
- Explain how to avoid unneccesary waste of resources on cloud platforms

### Exercise Part
- How many pixels are we looking at
- How many resources did we use up to this state
- How many snow covered pixels are there across all time steps?
- At which time step is the maximum snow cover reached? 
- What is the number of snow covered pixels on that date?
- What does that represent in area (km2)?
- How is the relation between snow cover and runoff?
  - Show different diagrams, the should choose the correct one

---
## Notes
### Topics
- Building an EO workflow with the available processes of a platform
- How openEO functions are defined (input, arguments, output)
- Data fusion
- Tracking resources

### Animations
- Put the full process graph as html to be inspected

### Exercises
- Perform the necessary steps of the snow workflow
- Probably needs to be split in multiple lectures
- Or the amount of predefined code is quite large
- Just an idea but we could let students choose a not yet computed area and combine the results and show on a combined map
- https://github.com/EO-College/cubes-and-clouds/blob/main/exercises/31_data_processing.ipynb

### Available Material
- EDC App: EO4Alps Snow
- Would be cool to link to an app that is deployed somewhere on a cloud

### Speakers
- METER Group, user perspective, potential use cases
- Any Commercial User of a cloud platform
